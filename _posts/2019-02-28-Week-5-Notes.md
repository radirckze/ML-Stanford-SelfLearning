---
layout: post
title:  "Week-5 notes."
author: "Ravi Dirckze"
---

#Lecture Notes - Neural Networks Learning

_**Work in progress. Working on this content**_ 

This page contains notes from video lectures for week 5.  

L: # of layers in the network.  
S<sub>l</sub>: # of units in layer l.  
K: # of output units

![W5 Image-1](https://radirckze.github.io/ML-Stanford-SelfLearning/assets/W5_Image1.png)

In the above network  
L = 4  
S<sub>2</sub> = 5  
S<sub>4</sub> = K = 4  

*Binary classification* => when K = 1, and as such h&Theta;(x) &isin; &real;

*Multi classification* => when K > 1, and as such h&Theta;(x) &isin; &real;<sup>K</sup>

The lines in light blue show the formula to calculate the 2nd node in L3, i.e., a<sub>2</sub><sup>(3)</sup> (ignoring the bias node which is not included in this diagram).

###Cost Function  

Recall, cost function for regularized logistic regression is:  

J(&theta;) = -1&frasl;m &sum;<sub>i=1</sub><sup>m</sup> [y<sup>(i)</sup> log(h<sub>&theta;</sub>(x<sup>(i)</sup>)) + (1 - y<sup>(i)</sup>)  log(1 - h<sub>&theta;</sub>(x<sup>(i)</sup>)) ] + &lambda;&frasl;2m  &sum;<sub>j=1</sub><sup>n</sup> &Theta;<sub>j</sub><sup>2</sup>

For neural networks the cost function is similar but more complicated. The neural network cost function can be described as:

J(&theta;) = For each training set (For each of the K elements (logistic regression cost function) ) + Regularization

That is, 

J(&theta;) = -1&frasl;m &sum;<sub>i=1</sub><sup>m</sup> &sum;<sub>k=1</sub><sup>K</sup> [y<sup>(i)</sup> log(h<sub>&theta;</sub>(x<sup>(i)</sup>)) + (1 - y<sup>(i)</sup>)  log(1 - h<sub>&theta;</sub>(x<sup>(i)</sup>)) ] + &lambda;&frasl;2m  &sum;<sub>l=1</sub><sup>L-1</sup> &sum;<sub>i=1</sub><sup>S<sub>l</sub></sup> &sum;<sub>j=1</sub><sup>s<sub>l+1</sub></sup> (&Theta;<sub>j,i</sub><sup>(l)</sup>)<sup>2</sup>

*Note* the regularization term for neural network cost function is simply a summation of all the &theta;'s used to generate values of each node in the subsequent layer, hence it goes from from 1 to L-1. Also note the subscript i in the regularization term is not related to the number of elements in the training sets. 
 
For example, looking at the above diagram (again, it does not contain bias nodes),  
a<sub>2</sub><sup>(3)</sup> = a<sub>1</sub><sup>(2)</sup> * &theta;<sub>2,1</sub><sup>(2)</sup> +...+ a<sub>5</sub><sup>(2)</sup> * &theta;<sub>2,5</sub><sup>(2)</sup>

Also, note that ((h&Theta;(x))<sub>i</sub> &equiv; i<sup>th</sup> output of where 1 &le; i &le; K


###Backpropagation Algorithm  

In neural networks we use Backpropagation to minimize the cost function, that is:  

min<sub>&Theta;</sub>J(&Theta;)

In order to use gradient descent or an advanced algorithm we need to compute:  

J(&Theta;), and  

&part; &frasl; (&part;(&Theta;)<sub>j,i</sub><sup>(l)</sup> )) J(&Theta;) 

_(Note: function for J(&Theta;) is listed above)_  

__Intuition for backpropagation algorithm__  

Recall,  given training example (x,y), the forward propagation algorith is:   

> a<sup>(1)</sup> = x  
> z<sup>(2)</sup> = &Theta;<sup>(1)</sup>a<sup>(1)</sup>  
> a<sup>(2)</sup> = g(z<sup>(2)</sup>) -- add a<sub>0</sub><sup>(2)</sup>, the bias node  
> z<sup>(3)</sup> = &Theta;<sup>(2)</sup>a<sup>(2)</sup>  
> a<sup>(3)</sup> = g(z<sup>(3)</sup>) -- add bias node  
> ....   
> a<sup>(L)</sup> = h<sub>&Theta;</sub>(x) = g(z<sup>(L-1)</sup>)  

Let &delta;<sub>j</sub><sup>(l)</sup> = error in the value of node j in layer l and a<sup>(l)</sup> be the calculated values in layer l.  

Since we know the expected output for a given training set which is (y) we can calculate &delta;<sub>j</sub><sup>(k)</sup>, that is:  
&delta;<sub>j</sub><sup>(k)</sup> = a<sub>j</sub><sup>(k)</sup> - y<sub>j</sub>  
We can vectorize the above for all j => &delta;<sup>(k)</sup> = a<sup>(k)</sup> - y.  
Moving back one layer we have:  
&delta;<sup>(k-1)</sup> = (&Theta;<sup>(k-1)</sup>)<sup>T</sup> &delta;<sup>k</sup> .* g&prime;(z<sup>(k-1)</sup>)  
where g is the zigmoid activation function. (Also note the element-wise .* operation).  So for l3:  
&delta;<sup>(3)</sup> = (&Theta;<sup>(3)</sup>)<sup>T</sup> &delta;<sup>4</sup> .* g&prime;(z<sup>(3)</sup>)

Using calculas it can be shown that:  
g&prime;(z<sup>(i)</sup>) = a<sup>(i)</sup> .* (1 - a<sup>(i)</sup>)  
where 1 is actually a vetor of 1's. So:  
g&prime;(z<sup>(3)</sup>) = a<sup>(3)</sup> .* (1 - a<sup>(3)</sup>)  

The term __back propagation__ comes from the fact that we use the known error to layer L to caclculate the error in each preceeding layer.

Through complicated mathematics (not provided in this course) it can be shown that:  

&part; &frasl; (&part;(&Theta;)<sub>j,i</sub><sup>(l)</sup> ) J(&Theta;) = a<sub>j</sub><sup>(l)</sup> &delta;<sub>i</sub><sup>(l+1)</sup>  -- ignoring &lambda; (regularication term) or &lambda; = 0

_Backpropagation Algorithm_  
> 
> Given training set (x,y) that is {(x<sup>(1)</sup>,y<sup>(1)</sup>),...., (x<sup>(m)</sup>,y<sup>(m)</sup>)}  
> 
> Set &Delta;<sub>i,j</sub><sup>(l)</sup> = 0 for all l,i,j. (So its an all 0 matrix)  
> 
> For i = 1 to m  
> &nbsp;&nbsp;&nbsp; set a<sup>(1)</sup> = x<sup>(1)</sup>  
> &nbsp;&nbsp;&nbsp; Perform forward propagation to compute a<sup>(l)</sup> for all l = 2...L  
> &nbsp;&nbsp;&nbsp; Using y<sup>(i)</sup> compute &delta;<sup>(L)</sup> = a<sup>(L)</sup> - y<sup>(i)</sup>  
> &nbsp;&nbsp;&nbsp; Compute &delta;<sup>(L-2)</sup>,..., &delta;<sup>(2)</sup> &nbsp;*- (see note BP1)*  
> &nbsp;&nbsp;&nbsp; &Delta;<sub>i,j</sub><sup>(l)</sup> = &Delta;<sub>i,j</sub><sup>(l)</sup> + a<sub>j</sub><sup>(l)</sup> &delta;<sub>i</sub><sup>(l+1)</sup> &nbsp;*- (see note BP2)*  
> End-for  
> 
> D<sub>i,j</sub><sup>(l)</sup> := (1 &frasl; m) &Delta;<sub>i,j</sub><sup>(l)</sup> + &lambda;&Theta;<sub>ij</sub><sup>(l)</sup> if j &ne; 0 &nbsp;*- (see note BP3)*  
> D<sub>i,j</sub><sup>(l)</sup> := (1 &frasl; m) &Delta;<sub>i,j</sub><sup>(l)</sup> if j = 0

_Note BP1:_ The error for layer 1 is zero so &delta;<sup>(1)</sup> is 0.  
_Note BP2:_ &nbsp;&nbsp;&nbsp; &Delta;<sub>i,j</sub><sup>(l)</sup> = &Delta;<sub>i,j</sub><sup>(l)</sup> + a<sub>j</sub><sup>(l)</sup> &delta;<sub>i</sub><sup>(l+1)</sup> can be vectorized as follows:  
&Delta;<sup>(l)</sup> = &Delta;<sup>(l)</sup> + &delta;<sup>(l+1)</sup>(a<sup>(l)</sup>)<sup>T</sup>  
_Note BP3:_ the D matrix is an accumulator to hold the partial derivatives as we compute then. As such we now have:  

&part; &frasl; (&part;(&Theta;)<sub>j,i</sub><sup>(l)</sup> ) J(&Theta;) = D<sub>i,j</sub><sup>{l}</sup>  

We now have both J(&Theta;) and &part; &frasl; (&part;(&Theta;)<sub>j,i</sub><sup>(l)</sup> ) J(&Theta;), and as such, have the values that we need to perform linear regression or use an advanced algorithm to minimize the cost function.


*__Note:__ No notes on the back-propagation intuition video. The calculation of the &delta; does not seem to be correct. In the forward propagation we use the sigmoid function but when calculating the &delta; in the &Theta; for each layer the sigmoid function is ignored. Regardless, what is shown here is not used nor necessary to solve the neural network.* 

###Loop Unrolling:  

Advanced optimization algorithms work with vectors so we need to turn our parameter matrices (i.e., &Theta;) into a vector.  

Recall from Week 3 Advanced Optimization Algorithms:

We first need to provide a function that evaluates the following two functions for a given input value Î¸:

J(&Theta;)  
&part; &frasl; &part;&Theta;<sub>j</sub> J(&Theta;)  

We can write a single function to calculate both these values:  
> function [jVal, gradient] = costFunction(theta)  
>   &nbsp;&nbsp;&nbsp; jVal = [...code to compute J(theta)...];  
>   &nbsp;&nbsp;&nbsp; gradient = [...code to compute derivative of J(theta)...];  
> end  

where the input parameter 'theta' and the output 'gradient' are both vectors.  

We can then call an advanced optimization algorithm (passing in the cost function):  
> [...] = fminunc(@costFunction, initialTheta, options);  

where 'initialTheta' is a vector as well. 

**Matrix rolling/unrolling**  

In network presented at the top we have S<sub>1</sub> = 3, S<sub>2</sub> = 5, S<sub>3</sub> = 6 and S<sub>4</sub> = 4.  
As such and ignoring the bias unit we would have:  
&Theta;<sup>(1)</sup> = 5x3, &Theta;<sup>(2)</sup> = 6x5 and &Theta;<sup>(3)</sup> = 4x6.  

In Octave we can convert the &Theta; matrices to a 1x69 vector (i.e., 15 + 30 + 24 = 69) as follows:  

> thetaVector = [ Theta1(:); Theta2(:); Theta3(:) ];  

To get back the matrices from the vector we use:  

Theta1 = reshape(thetaVector(1:15),5,3);  
Theta1 = reshape(thetaVector(16:45),6,5);  

**Updated Learning Algorithm**  

Using the above we incorporate matrices rolling/unrolling and rewrite the learning algorithms as follows:

initialThetaVector = unroll initial &Theta; matrices  

> function [jVal, gradientVec] = costFunction(thetaVector)  
>   &nbsp;&nbsp;&nbsp; from thetaVector use reshape to get &Theta;<sup>(1)</sup>, ..., &Theta;<sup>(L-1)</sup>.  
>   &nbsp;&nbsp;&nbsp; use forward prop/backward prop to compute D<sup>(1)</sup>, ..., D<sup>(L-1)</sup> and J(&Theta;)  
>   &nbsp;&nbsp;&nbsp; Unroll D<sup>(1)</sup>, ..., D<sup>(L-1)</Sup> to get gradientVec  
> end  

*Note: at some point, when I have looked at how advanced algorithms work, need to prove that using unrolled matrices will provice the correct result*  

###Gradient Checking  

The idea is to check the gradient calculated by back propagation at one point to ensure out algorithm is working properly. Assume that J(&Theta;) is a single value.  

![W5 Image-1](https://radirckze.github.io/ML-Stanford-SelfLearning/assets/NumEstOfGrad.png)  

The gradient at &Theta; can be estimated by picking a small value &epsilon; (say 10<sup>-4</sup> range), drawing a line between the two points corresponding to &Theta; - &epsilon; and &Theta; + &epsilon; and calculating its gradient. If we picked a small value for &epsilon; the gradient of the line should be pretty close to the gradient at &Theta; calculated by the back propagation algorithm.  

The gradient of the line between &Theta; - &epsilon; and &Theta; + &epsilon; is:

J(&Theta;+&epsilon;) - J(&Theta;-&epsilon;) / 2*&epsilon;  

If &Theta; is n dimensional (where &Theta; unrolled is [&Theta;<sub>1</sub>, ..., &Theta;<sub>n</sub>]) we can use the same idea to validate each of the &Theta;'s as follows:  

(&part;/&part;(&Theta;)<sub>m</sub>) J(&Theta;) &cong; (J(&Theta;<sub>1</sub>,..., &Theta;<sub>m</sub>+&epsilon;,..., &Theta;<sub>n</sub>) - J(&Theta;<sub>1</sub>,..., &Theta;<sub>m</sub>-&epsilon;,..., &Theta;<sub>n</sub>)) / 2 * &epsilon; for all n.  

So, to use the above grad-approximate to check the gradient descent of the back propagation neural network, we do the follwoing

> Implement back-propagation to calculate D<sup>1</sup>, ... , D<sup>n</sup>.  
> Calculate gradient-approximate and check that the values are similar.  
> Turn off gradient-approximate (as its very inefficient) and train the validated network.  


###Random Initialization 

For neural networks, initializing &Theta;'s to all zero (or even all 1) does not work as, this would make each element's effect on each element on the subsequent layer the same. (I am skipping the discussion on the details but this is supposed to result in a redundant ineffective network.)  

So the solution is to initialize each &Theta; to a value in the range -&epsilon;, +&epsilon; where &epsilon; is a small number close to zero (and is not related to the &epsilon; discussed above);  

To initialize a matrix M(x,y) to values between 0 and 1 use:  
M = rand(x,y);  
To To initialize a matrix M(x,y) to values between -&epsilon; and +&epsilon; use:  
M = rand(x,y) * 2 *  &epsilon; - &epsilon;;




---
__Markdown references__: for special characters see [this](https://brajeshwar.github.io/entities/). For Markdown cheat shess see [this](https://www.markdownguide.org/cheat-sheet/#basic-syntax).

 



