---
layout: post
title:  "Week-5 notes."
author: "Ravi Dirckze"
---

# Lecture Notes - Neural Networks Learning

_**Work in progress. Working on this content**_ 

This contains key notes from video lectures from week 5 *See paper notes for detailed lecture notes.*  

L: # of layers in the netowrk.  
S<sub>l</sub>: # of units in layer l.  
K: # of output units

![W5 Image-1](https://radirckze.github.io/ML-Stanford-SelfLearning/assets/W5_Image1.png)

In the above network  
L = 4  
S<sub>2</sub> = 5  
S<sub>4</sub> = K = 4  

*Binary classification* => when K = 1, and as such h&Theta;(x) &isin; &real;

*Multi classification* => when K > 1, and as such h&Theta;(x) &isin; &real;<sup>K</sup>

Recall, cost function for regularized logistic regression is:  

J(&theta;) = -1&frasl;m &sum;<sub>i=1</sub><sup>m</sup> [y<sup>(i)</sup> log(h<sub>&theta;</sub>(x<sup>(i)</sup>)) + (1 - y<sup>(i)</sup>)  log(1 - h<sub>&theta;</sub>(x<sup>(i)</sup>)) ] + &lambda;&frasl;2m  &sum;<sub>j=1</sub><sup>n</sup> &Theta;<sub>j</sub><sup>2</sup>

For neural networks the cost function is similar but more complicated. The neural network cost function can be described as:

J(&theta;) = For each training set (For each of the K elemens () losigtic regression cost function ) + Regularization

That is, 

J(&theta;) = -1&frasl;m &sum;<sub>i=1</sub><sup>m</sup> &sum;<sub>k=1</sub><sup>K</sup> [y<sup>(i)</sup> log(h<sub>&theta;</sub>(x<sup>(i)</sup>)) + (1 - y<sup>(i)</sup>)  log(1 - h<sub>&theta;</sub>(x<sup>(i)</sup>)) ] + &lambda;&frasl;2m  &sum;<sub>l=1</sub><sup>L-1</sup> &sum;<sub>i=1</sub><sup>S<sub>l</sub></sup> &sum;<sub>j=1</sub><sup>s<sub>l+1</sub></sup> (&Theta;<sub>j,i</sub><sup>(l)</sup>)<sup>2</sup>

*Note* the regularization term for neural network cost function is simply a summation of all the &theta;'s used to generate the terms of the subsequent layers from layer 1 to L-1. (*Note* subscript i in the regularization term is not related to training sets.) 
 
For example, looking at the above diagram (which does not contain bias nodes),  
a<sub>2</sub><sup>(3)</sup> = a<sub>1</sub><sup>(2)</sup> * &theta;<sub>2,1</sub><sup>(2)</sup> + ... + a<sub>5</sub><sup>(2)</sup> * &theta;<sub>2,5</sub><sup>(2)</sup>

Also, note that:

((h&Theta;(x))<sub>i</sub> &equiv; i<sup>th</sup> output of where 1 &le; i &le; K

(*editorial note%: review to make sure I got all the subscripts and superscripts righ.)  



---
__Markdown references__: for special characters see [this](https://brajeshwar.github.io/entities/). For Markdown cheat shess see [this](https://www.markdownguide.org/cheat-sheet/#basic-syntax).

  



