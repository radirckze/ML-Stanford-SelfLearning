---
layout: post
title:  "Week-5 notes."
author: "Ravi Dirckze"
---

# Lecture Notes - Neural Networks Learning

_**Work in progress. Working on this content**_ 

This contains key notes from video lectures from week 5 *See paper notes for detailed lecture notes.*  

L: # of layers in the netowrk.  
S<sub>l</sub>: # of units in layer l.  
K: # of output units

![W5 Image-1](https://radirckze.github.io/ML-Stanford-SelfLearning/assets/W5_Image1.png)

In the above network  
L = 4  
S<sub>2</sub> = 5  
S<sub>4</sub> = K = 4  

*Binary classification* => when K = 1, and as such h&Theta;(x) &isin; &real;

*Multi classification* => when K > 1, and as such h&Theta;(x) &isin; &real;<sup>K</sup>

The lines in light blue show the formula to calculate the 2nd node in L3, i.e., a<sub>2</sub><sup>(3)</sup> (ignoring the bias node which is not included in this diagram).

### Cost Function  

Recall, cost function for regularized logistic regression is:  

J(&theta;) = -1&frasl;m &sum;<sub>i=1</sub><sup>m</sup> [y<sup>(i)</sup> log(h<sub>&theta;</sub>(x<sup>(i)</sup>)) + (1 - y<sup>(i)</sup>)  log(1 - h<sub>&theta;</sub>(x<sup>(i)</sup>)) ] + &lambda;&frasl;2m  &sum;<sub>j=1</sub><sup>n</sup> &Theta;<sub>j</sub><sup>2</sup>

For neural networks the cost function is similar but more complicated. The neural network cost function can be described as:

J(&theta;) = For each training set (For each of the K elemens () losigtic regression cost function ) + Regularization

That is, 

J(&theta;) = -1&frasl;m &sum;<sub>i=1</sub><sup>m</sup> &sum;<sub>k=1</sub><sup>K</sup> [y<sup>(i)</sup> log(h<sub>&theta;</sub>(x<sup>(i)</sup>)) + (1 - y<sup>(i)</sup>)  log(1 - h<sub>&theta;</sub>(x<sup>(i)</sup>)) ] + &lambda;&frasl;2m  &sum;<sub>l=1</sub><sup>L-1</sup> &sum;<sub>i=1</sub><sup>S<sub>l</sub></sup> &sum;<sub>j=1</sub><sup>s<sub>l+1</sub></sup> (&Theta;<sub>j,i</sub><sup>(l)</sup>)<sup>2</sup>

*Note* the regularization term for neural network cost function is simply a summation of all the &theta;'s used to generate values of each node in the subsequent layer, hense it goes from from 1 to L-1. Also note the subscript i in the regularization term is not related to training sets. 
 
For example, looking at the above diagram (again, it does not contain bias nodes),  
a<sub>2</sub><sup>(3)</sup> = a<sub>1</sub><sup>(2)</sup> * &theta;<sub>2,1</sub><sup>(2)</sup> +...+ a<sub>5</sub><sup>(2)</sup> * &theta;<sub>2,5</sub><sup>(2)</sup>

Also, note that ((h&Theta;(x))<sub>i</sub> &equiv; i<sup>th</sup> output of where 1 &le; i &le; K

(**editorial note**: review to make sure I got all the subscripts and superscripts righ.)  

###Backpropegation Algorithm  

In neural networks we use Backpropagation to minimize the cost function, that is:  

min<sub>&Theta;</sub>J(&Theta;)

In order to use gradient descent or an advanced algorithm we need to compute:  

J(&Theta;), and  

&part; &frasl; &part;(&Theta;)<sub>j,i</sub><sup>(l)</sup> ) J(&Theta;) 

_(Note: function for J(&Theta;) is listed above)_  

__Intuition__  

Recall,  given training example (x,y), the forward propagation algorith is:   

> a<sup>(1)</sup> = x  
> z<sup>(2)</sup> = &Theta;<sup>(1)</sup>a<sup>(1)</sup>  
> a<sup>(2)</sup> = g(z<sup>(2)</sup>) -- add a<sub>0</sub><sup>(2)</sup>, the bias node  
> z<sup>(3)</sup> = &Theta;<sup>(2)</sup>a<sup>(2)</sup>  
> a<sup>(3)</sup> = g(z<sup>(3)</sup>) -- add bias node  
> ....   
> a<sup>(L)</sup> = h<sub>&Theta;</sub>(x) = g(z<sup>(L-1)</sup>)  

Let &delta;<sub>j</sub><sup>(l)</sup> = error in the value of node j in layer l and a<sup>(l)</sup> be the calculated values in layer l.  

Since we know the output for a given training set which is a<sup>(k)</sup> we can calculate &delta;<sub>j</sub><sup>(k)</sup>, that is:  
&delta;<sub>j</sub><sup>(k)</sup> = a<sub>j</sub><sup>(k)</sup> - y<sub>j</sub>  
We can vectorize the above for all j => &delta;<sup>(k)</sup> = a<sup>(k)</sup> - y.  
Moving back one layer we have:  
&delta;<sup>(k-1)</sup> = (&Theta;<sup>(k-1)</sup>)<sup>T</sup> &delta;<sup>k</sup> .* g&prime;(z<sup>(k-1)</sup>)  
where g is the zigmoid activation function. (Also note the element-wise .* operation).  So for l3:  
&delta;<sup>(3)</sup> = (&Theta;<sup>(3)</sup>)<sup>T</sup> &delta;<sup>4</sup> .* g&prime;(z<sup>(3)</sup>)

Using calculas it can be shown that:  
g&prime;(z<sup>(i)</sup>) = a<sup>(i)</sup> .* (1 - a<sup>(i)</sup>)  
where 1 is actually a vetor of 1's. So:  
g&prime;(z<sup>(3)</sup>) = a<sup>(3)</sup> .* (1 - a<sup>(3)</sup>)  

The term __back propagation__ comes from the fact that we use the known error to layer L to caclculate the error in each preceeding layer.

Through complicated mathematics (not provided in this course) it can be shown that:  

( &part; &frasl; &part;(&Theta;)<sub>j,i</sub><sup>(l)</sup> ) J(&Theta;) = a<sub>j</sub><sup>(l)</sup> &delta;<sub>i</sub><sup>(l+1)</sup>  -- ignoring &lambda; (regularication term) or &lambda; = 0

_Backpropagation Algorithm_  
> 
> Given training set (x,y) that is {(x<sup>(1)</sup>,y<sup>(1)</sup>),...., (x<sup>(m)</sup>,y<sup>(m)</sup>)}  
> 
> Set &Delta;<sub>i,j</sub><sup>(l)</sup> = 0 for all l,i,j. (So its an all 0 matrix)  
> 
> For i = 1 to m  
> &nbsp;&nbsp;&nbsp; set a<sup>(1)</sup> = x<sup>(1)</sup>  
> &nbsp;&nbsp;&nbsp; Perform forward propagation to compute a<sup>(l)</sup> for all l = 2...L  
> &nbsp;&nbsp;&nbsp; Using y<sup>(i)</sup> compute &delta;<sup>(L)</sup> = a<sup>(L)</sup> - y<sup>(i)</sup>  
> &nbsp;&nbsp;&nbsp; Compute &delta;<sup>(L-2)</sup>,..., &delta;<sup>(2)</sup> &nbsp;*- (see note BP1)*  
> &nbsp;&nbsp;&nbsp; &Delta;<sub>i,j</sub><sup>(l)</sup> = &Delta;<sub>i,j</sub><sup>(l)</sup> + a<sub>j</sub><sup>(l)</sup> &delta;<sub>i</sub><sup>(l+1)</sup> &nbsp;*- (see note BP2)*  
> End-for  
> 
> D<sub>i,j</sub><sup>(l)</sup> := (1 &frasl; m) &Delta;<sub>i,j</sub><sup>(l)</sup> + &lambda;&Theta;<sub>ij</sub><sup>(l)</sup> if j &ne; 0 &nbsp;*- (see note BP3)*  
> D<sub>i,j</sub><sup>(l)</sup> := (1 &frasl; m) &Delta;<sub>i,j</sub><sup>(l)</sup> if j = 0

_Note BP1:_ The error for layer 1 is zero so &delta;<sup>(1)</sup> is 0.  
_Note BP2:_ &nbsp;&nbsp;&nbsp; &Delta;<sub>i,j</sub><sup>(l)</sup> = &Delta;<sub>i,j</sub><sup>(l)</sup> + a<sub>j</sub><sup>(l)</sup> &delta;<sub>i</sub><sup>(l+1)</sup> can be vectorized as follows:  
&Delta;<sup>(l)</sup> = &Delta;<sup>(l)</sup> + &delta;<sup>(l+1)</sup>(a<sup>{i)</sup>)<sup>T</sup>  
_Note BP3:_ the D matrix is an accumulator to hold the partial derivates as we compute then. As such we now have:  

&part; &frasl; &part;(&Theta;)<sub>j,i</sub><sup>(l)</sup> ) J(&Theta;) = D<sub>i,j</sub><sup>{l}</sup>  

We now have both J(&Theta;) and &part; &frasl; &part;(&Theta;)<sub>j,i</sub><sup>(l)</sup> ) J(&Theta;), and as such, have the values that we need to perform linear regression or to use an adnvanced algorithm.



---
__Markdown references__: for special characters see [this](https://brajeshwar.github.io/entities/). For Markdown cheat shess see [this](https://www.markdownguide.org/cheat-sheet/#basic-syntax).

  



