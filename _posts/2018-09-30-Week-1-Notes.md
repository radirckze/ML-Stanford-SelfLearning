---
layout: post
title:  "Week-1 notes."
author: "Ravi Dirckze"
---

# Week 1: learning linear regression basics

### Part 1
Create a training set such that the optimal hypothesis (i.e., best fit linear equation) is known so that we can validate the results.

Graph1Data.txt constains a dataset based on the formula y = 1 + 0.5x. (Note: each point has been altered slightly to introduce randomness)


### Part 2
Guess at a linear equation and calculate the set of Y's for each X in the training set. Note that the linear equation is of the form:
y = theta0 + (theta1)x. 
Grapth the guessed linear equation (our hypothesis) agains the actual training set. 

Our initial guess is: theta0 = 1, theta1 = 1.

### Part 3
Plot the squared cost function. 

Pick a value for alpha and calculate 2 squared cost error values. If the result for the 2nd value is less than the first, the loop is progressing in the right direction. 

Calculate the squared cost function for some number of values (e.g., 30) and plot graph. It should be a bell curve.

Note: This step is not minimizing theta0 and theta1. They just illustrate that nature of the squared cost function. Its the nature of
this squared cost function (i.e., the bell curve) that enables us to find the best fitting hypothesis. 

**Why do we use the squared error cost function instead of the absolue cost function?**

See https://www.benkuhn.net/squared for details.

One of the key reasons is that "The squared error is everywhere differentiable, while the absolute error is not (its derivative is 
undefined at 0). This makes the squared error more amenable to the techniques of mathematical optimization. To optimize the squared error, you can just set its derivative equal to 0 and solve; to optimize the absolute error often requires more complex techniques."

(Another good source https://datascience.stackexchange.com/questions/10188/why-do-cost-functions-use-the-square-error)

### Part 4 
Plot hypothesis for the theta0 an theta1 at the bottom of the bell curve (i.e, the best fitting hypothesis for the graphed (theta0, theta1) pairs.

In this section I loop until the squared error stops decreasing, and then take the average  of the last to points as the estimation for theta0 and theta1.

Plot the training set, original hypothesis and hypothesis at the bottom of bell curve (i.e., the (theta0, theta1) pair that provided the least difference). 
The (theta0, theta1) pair that provided the least difference should be a better fit than the origina hypothesis, but note that its not necessarily the best fit.

### Part 5 
Use Gradient Descent to find best fit. Find the (theta0, theta1) pair that results in the least squared cost error.  Plot resulting graph and print optimal theta0 and theta1. Note that these values should very close to the values used to generate the training set data (i.e., y = 1 + 0.5x).


###Other Related notes

**Linear Regression**
In statistics, linear regression is a linear approach for modelling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X.  (https://en.wikipedia.org/wiki/Linear_regression)
Basically its a linear approach for modeling ...

**Gradient Descent**
Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. (https://en.wikipedia.org/wiki/Gradient_descent).

So, to summerize, Gradient Descent is an algorithm used to find the best fitting function for (possiblly only some) linear regression models. 



